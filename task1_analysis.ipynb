{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Classification Task\n",
    "\n",
    "## Objective\n",
    "Classify Yelp reviews into 1–5 stars using 3 different prompting approaches.\n",
    "Evaluate Accuracy, JSON Validity, and Reliability.\n",
    "\n",
    "## Setup\n",
    "Ensure `yelp.csv` is in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Mock API function - REPLACE THIS WITH ACTUAL API CALL CODE\n",
    "# e.g., using google-generativeai or openai\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    HAS_GENAI = True\n",
    "except ImportError:\n",
    "    HAS_GENAI = False\n",
    "\n",
    "API_KEY = os.getenv(\"GEMINI_API_KEY\") # Set this env var\n",
    "\n",
    "def call_llm(prompt, model_name=\"gemini-2.5-pro\"):\n",
    "    if HAS_GENAI and API_KEY:\n",
    "        genai.configure(api_key=API_KEY)\n",
    "        model = genai.GenerativeModel(model_name)\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    else:\n",
    "        # Mock response for testing without API Key\n",
    "        return '```json\\n{\"predicted_stars\": 4, \"explanation\": \"Simulated response because API_KEY is missing.\"}ֿ\\n```'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "df = pd.read_csv('yelp.csv')\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Sample 200 rows for efficiency, random_state for reproducibility\n",
    "sampled_df = df.sample(n=200, random_state=42).copy()\n",
    "print(f\"Sampled rows: {len(sampled_df)}\")\n",
    "sampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Prompts\n",
    "\n",
    "def clean_json_response(response_text):\n",
    "    # Attempt to extract JSON from markdown code blocks or raw text\n",
    "    match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return response_text\n",
    "\n",
    "# Approach 1: Zero-Shot (Direct Instruction)\n",
    "def prompt_zero_shot(review_text):\n",
    "    return f\"\"\"\n",
    "You are a sentiment analysis assistant. Analyze the following Yelp review and assign a star rating from 1 to 5.\n",
    "Return the output STRICTLY as a valid JSON object with the keys \"predicted_stars\" (integer) and \"explanation\" (string).\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "JSON:\n",
    "\"\"\"\n",
    "\n",
    "# Approach 2: Few-Shot (Providing Examples)\n",
    "def prompt_few_shot(review_text):\n",
    "    return f\"\"\"\n",
    "Classify the following Yelp review into 1-5 stars. Return JSON: {{\"predicted_stars\": <int>, \"explanation\": <str>}}.\n",
    "\n",
    "Example 1:\n",
    "Review: \"The service was terrible and the food was cold. Never coming back.\"\n",
    "Output: {{\"predicted_stars\": 1, \"explanation\": \"The reviewer expresses strong dissatisfaction with both service and food quality, indicating a very negative experience.\"}}\n",
    "\n",
    "Example 2:\n",
    "Review: \"It was okay, not great. The burger was good but the fries were soggy.\"\n",
    "Output: {{\"predicted_stars\": 3, \"explanation\": \"The reviewer has mixed feelings, praising the burger but criticizing the fries, suggesting an average experience.\"}}\n",
    "\n",
    "Example 3:\n",
    "Review: \"Absolutely amazing! Best pizza I've ever had. Staff was super friendly.\"\n",
    "Output: {{\"predicted_stars\": 5, \"explanation\": \"The reviewer uses superlatives like 'amazing' and 'best' and praises the staff, indicating a perfect experience.\"}}\n",
    "\n",
    "Task:\n",
    "Review: \"{review_text}\"\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "# Approach 3: Chain-of-Thought (Reasoning first)\n",
    "def prompt_cot(review_text):\n",
    "    return f\"\"\"\n",
    "Analyze the following Yelp review step-by-step to determine the appropriate star rating (1-5).\n",
    "1. Identify the key positive and negative sentiment markers in the text.\n",
    "2. Weigh the pros and cons mentions.\n",
    "3. Determine the overall sentiment score.\n",
    "4. Assign a star rating based on the overall sentiment.\n",
    "\n",
    "Finally, provide the result in JSON format: {{\"predicted_stars\": <int>, \"explanation\": <string>}}\n",
    "\n",
    "Review:\n",
    "\"{review_text}\"\n",
    "\n",
    "Analysis and JSON:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Execution Loop\n",
    "results = []\n",
    "\n",
    "dataset = sampled_df.to_dict('records')\n",
    "\n",
    "for i, row in enumerate(dataset):\n",
    "    review = row['text']\n",
    "    actual_stars = row['stars']\n",
    "    \n",
    "    # Run all 3 prompts\n",
    "    prompts = {\n",
    "        \"Zero-Shot\": prompt_zero_shot(review),\n",
    "        \"Few-Shot\": prompt_few_shot(review),\n",
    "        \"CoT\": prompt_cot(review)\n",
    "    }\n",
    "    \n",
    "    row_result = {\n",
    "        \"review_id\": row['review_id'],\n",
    "        \"actual_stars\": actual_stars\n",
    "    }\n",
    "    \n",
    "    for name, prompt_text in prompts.items():\n",
    "        try:\n",
    "            # Start timer\n",
    "            start_time = time.time()\n",
    "            raw_response = call_llm(prompt_text)\n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            # Parse JSON\n",
    "            json_str = clean_json_response(raw_response)\n",
    "            data = json.loads(json_str)\n",
    "            \n",
    "            row_result[f\"{name}_pred\"] = data.get('predicted_stars')\n",
    "            row_result[f\"{name}_valid\"] = True\n",
    "            row_result[f\"{name}_reason\"] = data.get('explanation')\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            row_result[f\"{name}_pred\"] = None\n",
    "            row_result[f\"{name}_valid\"] = False\n",
    "            row_result[f\"{name}_reason\"] = \"JSON Decode Error\"\n",
    "        except Exception as e:\n",
    "            row_result[f\"{name}_pred\"] = None\n",
    "            row_result[f\"{name}_valid\"] = False\n",
    "            row_result[f\"{name}_reason\"] = str(e)\n",
    "            \n",
    "    results.append(row_result)\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i}/{len(dataset)}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluation\n",
    "# Calculate accuracy, valid rate, etc.\n",
    "\n",
    "def calculate_metrics(df, method_name):\n",
    "    valid_rows = df[df[f\"{method_name}_valid\"] == True]\n",
    "    validity_rate = len(valid_rows) / len(df)\n",
    "    \n",
    "    if len(valid_rows) == 0:\n",
    "        return {\"Valid Rate\": validity_rate, \"Accuracy\": 0, \"MAE\": 0}\n",
    "        \n",
    "    # Accuracy (Exact match)\n",
    "    accuracy = (valid_rows[f\"{method_name}_pred\"] == valid_rows['actual_stars']).mean()\n",
    "    \n",
    "    # Mean Absolute Error\n",
    "    mae = (valid_rows[f\"{method_name}_pred\"] - valid_rows['actual_stars']).abs().mean()\n",
    "    \n",
    "    return {\n",
    "        \"Method\": method_name,\n",
    "        \"Valid Rate\": validity_rate,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"MAE\": mae\n",
    "    }\n",
    "\n",
    "metrics = []\n",
    "for method in [\"Zero-Shot\", \"Few-Shot\", \"CoT\"]:\n",
    "    metrics.append(calculate_metrics(results_df, method))\n",
    "    \n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "metrics_df.to_csv('metrics_summary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Review Classification Task\n",
    "\n",
    "## Objective\n",
    "Classify Yelp reviews into 1–5 stars using 3 different prompting approaches.\n",
    "Evaluate Accuracy, JSON Validity, and Reliability.\n",
    "\n",
    "## Setup\n",
    "Ensure `yelp.csv` is in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install -U google-genai pandas numpy tqdm\n",
    "\n",
    "import os, json, re, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Put your Gemini API key here or set it as an environment variable\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"YOUR_KEY\"\n",
    "\n",
    "from google import genai\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "client = genai.Client(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Load dataset (your attached file)\n",
    "df = pd.read_csv(\"yelp.csv\")\n",
    "df = df[df[\"type\"] == \"review\"].copy()  # safety; file has type column [file:1]\n",
    "df = df.dropna(subset=[\"text\", \"stars\"]).reset_index(drop=True)  # [file:1]\n",
    "\n",
    "# Sample ~200 rows for evaluation\n",
    "SAMPLE_N = 200\n",
    "SEED = 42\n",
    "eval_df = df.sample(n=SAMPLE_N, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "eval_df[[\"stars\", \"text\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_V1 = \"\"\"\n",
    "Classify the Yelp review into an integer star rating from 1 to 5.\n",
    "\n",
    "Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"predicted_stars\": 4,\n",
    "  \"explanation\": \"Brief reasoning for the assigned rating.\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT_V2 = \"\"\"\n",
    "You are a strict Yelp star-rating classifier.\n",
    "\n",
    "Use this rubric:\n",
    "- 1 star: terrible experience, strong complaints, would not return.\n",
    "- 2 stars: below average, several issues, disappointed.\n",
    "- 3 stars: mixed/average, pros and cons, acceptable.\n",
    "- 4 stars: good experience, minor issues, would return.\n",
    "- 5 stars: excellent, enthusiastic praise, highly recommend.\n",
    "\n",
    "Return ONLY valid JSON exactly:\n",
    "{{\n",
    "  \"predicted_stars\": <1-5 integer>,\n",
    "  \"explanation\": \"One short sentence justifying the rating.\"\n",
    "}}\n",
    "\n",
    "Review:\n",
    "{review}\n",
    "\"\"\".strip()\n",
    "\n",
    "PROMPT_V3 = \"\"\"\n",
    "You classify Yelp reviews into 1–5 stars.\n",
    "\n",
    "Rules:\n",
    "- Output MUST be valid JSON only (no markdown, no extra text).\n",
    "- Output MUST contain exactly two keys: predicted_stars, explanation.\n",
    "- predicted_stars must be an integer 1..5.\n",
    "\n",
    "Examples:\n",
    "Review: \"Absolutely amazing service and the food was perfect. Can't wait to come back!\"\n",
    "Output: {{\"predicted_stars\": 5, \"explanation\": \"Strong praise and eagerness to return indicates an excellent experience.\"}}\n",
    "\n",
    "Review: \"It was okay. Some things were good, but the wait was long and the place felt average.\"\n",
    "Output: {{\"predicted_stars\": 3, \"explanation\": \"Mixed feedback with notable downsides fits an average experience.\"}}\n",
    "\n",
    "Review: \"Never again. Rude staff, cold food, and it took forever.\"\n",
    "Output: {{\"predicted_stars\": 1, \"explanation\": \"Severe complaints and refusal to return indicate a very poor experience.\"}}\n",
    "\n",
    "Now classify:\n",
    "Review: {review}\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gemini-2.5-flash\"  # fast/cheap for batch eval; switch to pro if needed\n",
    "\n",
    "def call_gemini(prompt: str, temperature=0.2, max_retries=3, retry_sleep=1.0):\n",
    "    \"\"\"\n",
    "    Calls Gemini and returns raw text.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = client.models.generate_content(\n",
    "                model=MODEL,\n",
    "                contents=prompt,\n",
    "                config=genai.types.GenerateContentConfig(\n",
    "                    temperature=temperature,\n",
    "                    # Keep outputs short to reduce JSON breakage\n",
    "                    max_output_tokens=120,\n",
    "                )\n",
    "            )\n",
    "            return resp.text\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                return f\"__ERROR__:{repr(e)}\"\n",
    "            time.sleep(retry_sleep * (attempt + 1))\n",
    "\n",
    "def extract_json_str(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Attempts to extract a JSON object substring if the model adds extra text.\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.strip()\n",
    "    # If already looks like JSON\n",
    "    if s.startswith(\"{\") and s.endswith(\"}\"):\n",
    "        return s\n",
    "    # Fallback: find first {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", s, flags=re.DOTALL)\n",
    "    return m.group(0).strip() if m else s\n",
    "\n",
    "def parse_prediction(raw: str):\n",
    "    \"\"\"\n",
    "    Returns: (is_valid_json, predicted_stars_or_none, explanation_or_none, parsed_obj_or_none)\n",
    "    \"\"\"\n",
    "    if raw.startswith(\"__ERROR__\"):\n",
    "        return False, None, None, None\n",
    "\n",
    "    j = extract_json_str(raw)\n",
    "    try:\n",
    "        obj = json.loads(j)\n",
    "        # Validate schema\n",
    "        if not isinstance(obj, dict):\n",
    "            return False, None, None, None\n",
    "        if set(obj.keys()) != {\"predicted_stars\", \"explanation\"}:\n",
    "            return False, None, None, obj\n",
    "        ps = obj[\"predicted_stars\"]\n",
    "        ex = obj[\"explanation\"]\n",
    "        if not isinstance(ps, int) or ps < 1 or ps > 5:\n",
    "            return False, None, None, obj\n",
    "        if not isinstance(ex, str) or len(ex.strip()) == 0:\n",
    "            return False, None, None, obj\n",
    "        return True, ps, ex.strip(), obj\n",
    "    except Exception:\n",
    "        return False, None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(prompt_template: str, name: str, temperature=0.2):\n",
    "    rows = []\n",
    "    for i in tqdm(range(len(eval_df)), desc=f\"Running {name}\"):\n",
    "        review = eval_df.loc[i, \"text\"]\n",
    "        actual = int(eval_df.loc[i, \"stars\"])\n",
    "        prompt = prompt_template.format(review=review)\n",
    "\n",
    "        raw = call_gemini(prompt, temperature=temperature)\n",
    "        is_valid, pred, expl, _ = parse_prediction(raw)\n",
    "\n",
    "        rows.append({\n",
    "            \"approach\": name,\n",
    "            \"row_id\": i,\n",
    "            \"actual_stars\": actual,\n",
    "            \"raw_output\": raw,\n",
    "            \"json_valid\": is_valid,\n",
    "            \"predicted_stars\": pred,\n",
    "            \"explanation\": expl,\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    # Valid rows = valid JSON and a numeric prediction\n",
    "    valid_mask = out[\"json_valid\"] & out[\"predicted_stars\"].notna()\n",
    "    out[\"correct\"] = False  # default for invalid rows\n",
    "\n",
    "    # Only cast on filtered rows (so no None enters astype(int))\n",
    "    out.loc[valid_mask, \"correct\"] = (\n",
    "        out.loc[valid_mask, \"predicted_stars\"].astype(int).values\n",
    "        == out.loc[valid_mask, \"actual_stars\"].astype(int).values\n",
    "    )\n",
    "\n",
    "    accuracy = out.loc[valid_mask, \"correct\"].mean() if valid_mask.any() else 0.0\n",
    "    json_valid_rate = out[\"json_valid\"].mean()\n",
    "\n",
    "    return out, accuracy, json_valid_rate\n",
    "\n",
    "\n",
    "res_v1, acc_v1, jvr_v1 = evaluate_prompt(PROMPT_V1, \"V1_simple\", temperature=0.2)\n",
    "res_v2, acc_v2, jvr_v2 = evaluate_prompt(PROMPT_V2, \"V2_rubric\", temperature=0.2)\n",
    "res_v3, acc_v3, jvr_v3 = evaluate_prompt(PROMPT_V3, \"V3_fewshot\", temperature=0.2)\n",
    "\n",
    "acc_v1, jvr_v1, acc_v2, jvr_v2, acc_v3, jvr_v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reliability_test(prompt_template: str, name: str, temperature=0.2):\n",
    "    r1, _, _ = evaluate_prompt(prompt_template, name + \"_run1\", temperature=temperature)\n",
    "    r2, _, _ = evaluate_prompt(prompt_template, name + \"_run2\", temperature=temperature)\n",
    "\n",
    "    merged = r1.merge(r2, on=\"row_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    # Only count rows where both outputs were valid JSON\n",
    "    both_valid = merged[\"json_valid_1\"] & merged[\"json_valid_2\"]\n",
    "    # Consistency = same predicted stars when both valid\n",
    "    consistency = (merged.loc[both_valid, \"predicted_stars_1\"] == merged.loc[both_valid, \"predicted_stars_2\"]).mean() if both_valid.any() else 0.0\n",
    "    return consistency, merged\n",
    "\n",
    "cons_v1, _ = reliability_test(PROMPT_V1, \"V1_simple\", temperature=0.2)\n",
    "cons_v2, _ = reliability_test(PROMPT_V2, \"V2_rubric\", temperature=0.2)\n",
    "cons_v3, _ = reliability_test(PROMPT_V3, \"V3_fewshot\", temperature=0.2)\n",
    "\n",
    "cons_v1, cons_v2, cons_v3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame([\n",
    "    {\"approach\": \"V1_simple\", \"accuracy_on_valid\": acc_v1, \"json_valid_rate\": jvr_v1, \"consistency_2runs\": cons_v1},\n",
    "    {\"approach\": \"V2_rubric\", \"accuracy_on_valid\": acc_v2, \"json_valid_rate\": jvr_v2, \"consistency_2runs\": cons_v2},\n",
    "    {\"approach\": \"V3_fewshot\", \"accuracy_on_valid\": acc_v3, \"json_valid_rate\": jvr_v3, \"consistency_2runs\": cons_v3},\n",
    "])\n",
    "\n",
    "comparison.sort_values(by=[\"accuracy_on_valid\", \"json_valid_rate\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "Short discussion (edit after seeing your numbers):\n",
    "- V1 (simple) is a baseline; it may drift or output non-JSON occasionally.\n",
    "- V2 (rubric) usually improves rating calibration by anchoring what each star means.\n",
    "- V3 (few-shot + strict JSON rules) often improves JSON validity and reduces formatting errors; it may also improve accuracy by demonstrating the target style.\n",
    "\"\"\")\n",
    "all_results = pd.concat([res_v1, res_v2, res_v3], ignore_index=True)\n",
    "all_results.to_csv(\"task1_gemini_results.csv\", index=False)\n",
    "comparison.to_csv(\"task1_gemini_comparison.csv\", index=False)\n",
    "\n",
    "all_results.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
